# ğŸ§  Engram

**Engram** - A passive memory layer for AI conversations. Automatically injects relevant memories into LLM context and extracts new insights from responses.

[![Engram Demo](https://img.youtube.com/vi/7J_UNgNKp7g/0.jpg)](https://youtu.be/7J_UNgNKp7g)

> *An engram is the physical trace of a memory in the brain.*

## âœ¨ Key Features

- **Transparent Memory**: Memory is injected and extracted without LLM awareness
- **Intelligent MemMan Agent**: Background LLM agent that analyzes conversations and decides what to remember
- **Semantic Search**: FAISS-based vector storage with sentence transformer embeddings
- **Async Extraction**: Background memory extraction doesn't block conversation flow
- **Memory Reinforcement**: Similar memories are reinforced, increasing importance over time
- **Context-Aware**: Multi-factor relevance scoring (importance, recency, usage patterns)
- **Local-First**: All data stored locally, works offline after initial setup

## ğŸ“¦ Installation

```bash
cd engram
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

Set your API key:
```bash
export GEMINI_API_KEY=your-api-key
# Or place in .env file
```

## ğŸš€ Quick Start

### Interactive Chat

```bash
python brain.py
```

### CLI Commands

```bash
# Search memories
python brain.py --search "python patterns"

# Add a memory manually
python brain.py --add "Always use type hints in function signatures"

# Remove a memory by ID
python brain.py --remove abc123

# Show statistics
python brain.py --stats
```

### In-Chat Commands

```
/help          Show available commands
/memories      Search your memories
/recent        Show recent memories
/stats         Show session statistics
/add <text>    Manually add a memory
/quit          Exit the chat
```

### Memory Visualizer

Monitor memory state in real-time during conversations:

```bash
# In a separate terminal
python memory_visualizer.py
```

**Sort Modes:**
```bash
python memory_visualizer.py                      # Combined ranking (default)
python memory_visualizer.py --sort importance    # By importance score
python memory_visualizer.py --sort recency       # By timestamp
python memory_visualizer.py --sort access        # By access count
python memory_visualizer.py --query "topic"      # By relevance to query
```

**Keyboard Controls:**
- `q` - Quit
- `1` - Sort by importance
- `2` - Sort by recency
- `3` - Sort by access count
- `4` - Sort by combined score
- `5` - Sort by relevance

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     User Message                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PassiveMemoryProxy                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ 1. RETRIEVE (sync)  â”‚    â”‚ Vector Search (~10ms)    â”‚   â”‚
â”‚  â”‚    Search memories  â”‚â”€â”€â”€â–¶â”‚ Get relevant memories    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ 2. INJECT (sync)    â”‚    â”‚ Context Formatting       â”‚   â”‚
â”‚  â”‚    Build prompt     â”‚â”€â”€â”€â–¶â”‚ Add memories to system   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ 3. CALL LLM (sync)  â”‚    â”‚ Gemini API              â”‚   â”‚
â”‚  â”‚    Get response     â”‚â”€â”€â”€â–¶â”‚ Memory-enhanced prompt   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ 4. EXTRACT (async)  â”‚    â”‚ MemMan Agent (LLM)       â”‚   â”‚
â”‚  â”‚    Queue extraction â”‚â”€â”€â”€â–¶â”‚ Analyze & store memories â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Assistant Response                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Components

| Component | File | Purpose |
|-----------|------|---------|
| `PassiveMemoryProxy` | `memory_proxy.py` | Transparent LLM proxy with memory injection |
| `MemoryExtractor` | `memory_extractor.py` | **MemMan Agent** - async LLM-powered memory management |
| `VectorMemory` | `engram_pkg/core.py` | FAISS vector storage and semantic search |
| `MemoryContextIntegrator` | `memory_context.py` | Context-aware retrieval and scoring |
| `ContextWindowManager` | `context_window_manager.py` | Token budget management |
| `MemoryVisualizer` | `memory_visualizer.py` | Real-time CLI memory visualization |

### ğŸ¤– MemMan Agent

The **MemMan (Memory Manager) Agent** is a background LLM-powered worker that intelligently manages memory extraction:

- **Async Processing**: Runs in a separate thread, never blocking the main conversation
- **LLM Intelligence**: Uses a fast/cheap model (Gemini Flash Lite) to analyze each exchange
- **Smart Filtering**: Decides what's actually worth remembering vs. transient chatter
- **Memory Types**: Extracts preferences, facts, decisions, and insights
- **Confidence Scoring**: Assigns importance and confidence to each memory
- **Memory Reinforcement**: When similar memories are detected, reinforces them (increases importance and access count)
- **Graceful Fallback**: Falls back to heuristic extraction if LLM is unavailable

MemMan output appears in real-time during chat:
```
MemMan: ğŸ’¾ New (imp:0.70): User prefers dark mode in applications...
MemMan: ğŸ”„ Reinforced (acc:3, imp:0.65â†’0.72): Working on ProjectX...
```

## ğŸ“Š Memory Schema

```python
@dataclass
class MemoryEntry:
    id: str                    # Unique identifier
    content: str               # Memory content
    timestamp: datetime        # Creation time
    importance: float          # 0.0 to 1.0
    tags: List[str]           # Categorization tags
    context: Dict[str, Any]   # Additional metadata
    access_count: int         # Usage tracking
    last_accessed: datetime   # Last retrieval time
    embedding: List[float]    # Vector representation
```

## ğŸ”§ Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `GEMINI_API_KEY` | Google Gemini API key | Required |

### ProxyConfig Options

```python
from memory_proxy import PassiveMemoryProxy, ProxyConfig

config = ProxyConfig(
    memory_path="vector_memory",        # Storage location
    max_memories_to_inject=5,           # Memories per query
    min_memory_importance=0.2,          # Minimum importance threshold
    model="gemini-2.0-flash",           # Main LLM model
    extraction_model="gemini-2.0-flash-lite",  # Extraction model
    memory_token_budget=1000,           # Max tokens for memory context
    extraction_enabled=True,            # Enable async extraction
    verbose=False                       # Debug logging
)

proxy = PassiveMemoryProxy(config=config)
```

## ğŸ“ Project Structure

```
engram/
â”œâ”€â”€ brain.py              # CLI chat interface
â”œâ”€â”€ memory_proxy.py       # Main proxy (use this!)
â”œâ”€â”€ memory_extractor.py   # MemMan Agent - async LLM memory management
â”œâ”€â”€ memory_integration.py # Memory integration layer
â”œâ”€â”€ memory_context.py     # Context-aware retrieval
â”œâ”€â”€ memory_visualizer.py  # Real-time memory TUI
â”œâ”€â”€ context_window_manager.py # Token management
â”œâ”€â”€ engram_pkg/           # Core package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ core.py           # VectorMemory class
â”‚   â”œâ”€â”€ context.py
â”‚   â”œâ”€â”€ integration.py
â”‚   â””â”€â”€ cli.py
â”œâ”€â”€ vector_memory/        # Data storage
â”‚   â”œâ”€â”€ metadata.pkl
â”‚   â””â”€â”€ faiss_index.bin
â””â”€â”€ requirements.txt
```

## ğŸ“ License

MIT License
